---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a theoretical physicist with a broad interest in physics, mathematics, and
computer science. After completing my PhD in theoretical physics in 2021, I
worked as a postdoctoral researcher developing tools to study complex systems. I
am now transitioning to the field of machine learning, where I believe I can
make a more impactful contribution by applying my analytical skills and
scientific background. You can find a more detailed description of my skills and
employement history on my [CV](files/cv.pdf).

### Physics
My research in physics is motivated by understanding the properties of
strongly-coupled systems by studying extended objects known as defects.
This research direction aims to develop analytical tools to characterise complex
systems. These tools provide a window into the dynamics of such systems.
As an example, a long-standing question in the field is to understand the
mechanism behind confinement in the theory of quantum chromodynamics (QCD), and
the relevant defect in this case is called a Wilson line.
In my research, I have developed tools to study surface defects (which are
similar to Wilson lines) in models resembling QCD but with better analytical
control.

More specifically, in a [series of projects](publications/) I have developped
tools to construct and calculate properties of surface defects in the context of
field theory, holography, the conformal bootstrap and supersymmetry. In
particular I have played a leading role in applying new techniques to a
mysterious strongly-coupled 6d theory predicted by string theory, known as the
6d $\mathcal{N} = (2,0)$ theory.

### Machine learning

Modern AI systems are transforming how we approach and process information, from
answering complex queries to summarising documents or capturing relevant
dataset features. The key behind these developments is the ability of deep neural
networks (DNN) to effectively extract semantic features from large datasets.

This transformation presents several interesting challenges to overcome.
I'm particularly excited by the following lines of research:
 - developping insights into DNN workings using physics tools, e.g.
   [The Principles of Deep Learning Theory](https://arxiv.org/abs/2106.10165)
 - using and explaining empirical scaling laws to traing models efficiently,
   e.g. [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
 - developing efficient prunning strategies
 - developping interpretability techniques to make models more transparent and
   trustworthy, e.g. [Scaling Monosemanticity: Extracting Interpretable Features
   from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

To better understand the universality of scaling laws, I recently studied them
within the simplified context of gaussian process regression. You can read about
it [here](https://github.com/maxtrepanier/scaling_law_gp/blob/main/Scaling%20laws%20in%20gaussian%20processes.ipynb)!

In addition to these research directions, there are many interesting
applications within reach. For instance, I recently built an [accent
classifier](https://github.com/maxtrepanier/whisper-accent/blob/master/Accent%20detection.ipynb)
to explore the capabilities of state-of-the-art speech recognition model
Whisper at capturing subtle features of speech like accents.  The classifier's
high accuracy suggests that Whisper implicitly learns features interpretable as
regional accents.

Finally, I'm keen to explore bayesian methods in the context of sensitive
applications, and the tools of reinforcement and self-supervised learning.
